{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word Cloud From url",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUjG7tIpfsHw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fh8_cpTar8vh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = input(\"Enter your url here: \\n\")\n",
        "\n",
        "news_html = requests.get(url).content\n",
        "news_soup = BeautifulSoup(news_html , 'lxml')\n",
        "paragraphs = [par.text for par in news_soup.find_all('p')]\n",
        "text = '\\n'.join(paragraphs)\n",
        "\n",
        "# break into lines and remove leading and trailing space on each\n",
        "lines = (line.strip() for line in text.splitlines())\n",
        "\n",
        "# break multi-headlines into a line each\n",
        "chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "\n",
        "# drop blank lines\n",
        "text = 'n'.join(chunk for chunk in chunks if chunk)\n",
        "\n",
        "\n",
        "#download and print the stop words for the English language\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "#tokenise the data set\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "words = word_tokenize(text)\n",
        "\n",
        "# removes punctuation and numbers\n",
        "wordsFiltered = [word.lower() for word in words if word.isalpha()]\n",
        "\n",
        "# remove stop words from tokenised data set\n",
        "filtered_words = [word for word in wordsFiltered if word not in stopwords.words('english')]\n",
        "\n",
        "#ploting word cloud\n",
        "wc = WordCloud(max_words=30, margin=10, background_color='white', scale=3, relative_scaling = 0.5, \\\n",
        "               width=500, height=400).generate(' '.join(filtered_words))\n",
        "plt.figure(figsize=(10,16))\n",
        "plt.imshow(wc)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
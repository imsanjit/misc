{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Get Breadcrumbs of page from urls",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4N6i-o13jksw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "\n",
        "csv_file = open('Breadcrumbs.csv', 'w')\n",
        "csv_writer = csv.writer(csv_file)\n",
        "csv_writer.writerow(['URL', 'Breadcrumbs '])\n",
        "\n",
        "\n",
        "url_in=input('Kindly Enter a url or list of urls sepatared by commams:  ').split(sep=' ')\n",
        "\n",
        "\n",
        "for x in url_in:\n",
        "  response=requests.get(x)\n",
        "  if response.status_code  == 200:\n",
        "    try:\n",
        "      html_soup = BeautifulSoup(response.text, 'html.parser')\n",
        "      s = []\n",
        "      \n",
        "      for x in html_soup.find(\"div\", class_ = \"breadcrumb-section clearfix\").find_all('span'):\n",
        "        s.append(x.get_text(strip=True))\n",
        "        \n",
        "      csv_writer.writerow([x, s])\n",
        "\n",
        "\n",
        "\n",
        "    except:\n",
        "      pass\n",
        "    print(f'Working on : {x}')\n",
        "    \n",
        "csv_file.close() \n",
        "print('ALL DONE') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hHhxo_2IWVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "\n",
        "csv_file = open('Breadcrumbs.csv', 'w')\n",
        "csv_writer = csv.writer(csv_file)\n",
        "csv_writer.writerow(['URL', 'Breadcrumbs '])\n",
        "\n",
        "\n",
        "url_in=input('Kindly Enter a url or list of urls sepatared by commams:  ').split(sep=' ')\n",
        "\n",
        "\n",
        "for i, x in enumerate(url_in):\n",
        "  response=requests.get(x)\n",
        "  if response.status_code  == 200:\n",
        "    try:\n",
        "      html_soup = BeautifulSoup(response.text, 'html.parser')\n",
        "      s = []\n",
        "      for sanjit in html_soup.find(\"div\", class_ = \"breadcrumb-section clearfix\").find_all('span'):\n",
        "        s.append(sanjit.get_text(strip=True))\n",
        "      csv_writer.writerow([x, s])\n",
        "\n",
        "\n",
        "\n",
        "    except:\n",
        "      pass\n",
        "    print(f'Working on url :{i+1} : {x}')\n",
        "    \n",
        "csv_file.close() \n",
        "print('ALL DONE. Download your file....') "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Extract All Headings From Web Page Thru urls (multiple urls) ",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7Q_6jgm6Oau",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get Headings in single columns (using regex)\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import re\n",
        "\n",
        "csv_file = open('headings.csv', 'w')\n",
        "csv_writer = csv.writer(csv_file)\n",
        "csv_writer.writerow(['URL', 'H'])\n",
        "\n",
        "url_in=input('Kindly Enter a url or list of urls sepatared by commams:  ').split(sep=' ')\n",
        "\n",
        "for x in url_in:\n",
        "  response=requests.get(x)\n",
        "  if response.status_code  == 200:\n",
        "    try:\n",
        "      html_soup = BeautifulSoup(response.text, 'html.parser')\n",
        "      h =html_soup.find(\"div\", class_ = \"tab-content\").find_all(re.compile(r'h\\d+'))\n",
        "      csv_writer.writerow([x, h])\n",
        "    \n",
        "    except:\n",
        "      pass\n",
        "\n",
        "csv_file.close()  \n",
        "\n",
        "print('ALL DONE') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxCQaDa86PoY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get Headings in different columns (using tags)\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "\n",
        "csv_file = open('headings.csv', 'w')\n",
        "csv_writer = csv.writer(csv_file)\n",
        "csv_writer.writerow(['URL', 'H1', 'H2', 'H3', 'H4', 'H5', 'H6'])\n",
        "\n",
        "url_in=input('Kindly Enter a url or list of urls sepatared by commams:  ').split(sep=' ')\n",
        "\n",
        "for x in url_in:\n",
        "  response=requests.get(x)\n",
        "  if response.status_code  == 200:\n",
        "    try:\n",
        "      html_soup = BeautifulSoup(response.text, 'html.parser')\n",
        "      h =html_soup.find(\"div\", class_ = \"tab-content\")\n",
        "      h1 =h.find_all(\"h1\")\n",
        "      h2 =h.find_all(\"h2\")\n",
        "      h3 =h.find_all(\"h3\")\n",
        "      h4 =h.find_all(\"h4\")\n",
        "      h5 =h.find_all(\"h5\")\n",
        "      h6 =h.find_all(\"h6\")\n",
        "      csv_writer.writerow([x, h1, h2, h3, h4, h5, h6])\n",
        "\n",
        "    except:\n",
        "      pass\n",
        "      \n",
        "      \n",
        "csv_file.close()  \n",
        "\n",
        "print('ALL DONE') "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}